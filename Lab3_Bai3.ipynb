{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Bài 3"
      ],
      "metadata": {
        "id": "Rotv3ZlVVtmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTCNF-bFV3aq",
        "outputId": "31b6b8ac-4fa5-43b8-fdbe-a7cacbe49caa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.12/dist-packages (from seqeval) (1.6.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=728fc5f0d3847ea540b804dc34da969a5f5e780e2b27da26d2be3b8eaf240ff9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from seqeval.metrics import f1_score as ner_f1_score\n",
        "from seqeval.metrics import classification_report as ner_report\n",
        "from datasets import disable_progress_bar\n",
        "from datasets import utils\n",
        "\n",
        "disable_progress_bar()\n",
        "utils.logging.set_verbosity_error()\n",
        "\n",
        "data_files = {\n",
        "    \"train\": \"https://raw.githubusercontent.com/VinAIResearch/PhoNER_COVID19/main/data/word/train_word.json\",\n",
        "    \"validation\": \"https://raw.githubusercontent.com/VinAIResearch/PhoNER_COVID19/main/data/word/dev_word.json\",\n",
        "    \"test\": \"https://raw.githubusercontent.com/VinAIResearch/PhoNER_COVID19/main/data/word/test_word.json\"\n",
        "}\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "train_sentences = dataset[\"train\"][\"words\"]\n",
        "dev_sentences   = dataset[\"validation\"][\"words\"]\n",
        "test_sentences  = dataset[\"test\"][\"words\"]\n",
        "\n",
        "train_tags_raw = dataset[\"train\"][\"tags\"]\n",
        "dev_tags_raw   = dataset[\"validation\"][\"tags\"]\n",
        "test_tags_raw  = dataset[\"test\"][\"tags\"]\n",
        "\n",
        "unique_tags = set(tag for tags in train_tags_raw for tag in tags)\n",
        "tag_names = sorted(list(unique_tags))\n",
        "num_tags = len(tag_names)\n",
        "\n",
        "tag2id = {tag: i for i, tag in enumerate(tag_names)}\n",
        "\n",
        "def convert_tags_to_ids(tags_list_raw):\n",
        "    return [[tag2id[tag] for tag in tags] for tags in tags_list_raw]\n",
        "\n",
        "train_tags = convert_tags_to_ids(train_tags_raw)\n",
        "dev_tags   = convert_tags_to_ids(dev_tags_raw)\n",
        "test_tags  = convert_tags_to_ids(test_tags_raw)\n",
        "\n",
        "print(f\"Số lượng nhãn: {num_tags}\")\n",
        "print(f\"Danh sách nhãn: {tag_names}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFWnyIY7V4dh",
        "outputId": "c5f06d9e-005e-41d5-fd1d-674acdeb7f50"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số lượng nhãn: 20\n",
            "Danh sách nhãn: ['B-AGE', 'B-DATE', 'B-GENDER', 'B-JOB', 'B-LOCATION', 'B-NAME', 'B-ORGANIZATION', 'B-PATIENT_ID', 'B-SYMPTOM_AND_DISEASE', 'B-TRANSPORTATION', 'I-AGE', 'I-DATE', 'I-JOB', 'I-LOCATION', 'I-NAME', 'I-ORGANIZATION', 'I-PATIENT_ID', 'I-SYMPTOM_AND_DISEASE', 'I-TRANSPORTATION', 'O']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_set = set()\n",
        "for sentence in train_sentences:\n",
        "    for word in sentence:\n",
        "        word_set.add(word)\n",
        "\n",
        "vocab = {word: i+2 for i, word in enumerate(word_set)}\n",
        "vocab[\"<PAD>\"] = 0\n",
        "vocab[\"<UNK>\"] = 1\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Kích thước từ điển: {vocab_size}\")\n",
        "\n",
        "class NerDataset(Dataset):\n",
        "    def __init__(self, sentences, tags, vocab, max_len=100):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        token_list = self.sentences[idx]\n",
        "        tag_list = self.tags[idx]\n",
        "\n",
        "        token_ids = [self.vocab.get(w, self.vocab[\"<UNK>\"]) for w in token_list]\n",
        "\n",
        "        if len(token_ids) < self.max_len:\n",
        "            pad_len = self.max_len - len(token_ids)\n",
        "            token_ids = token_ids + [self.vocab[\"<PAD>\"]] * pad_len\n",
        "            tag_list  = tag_list  + [-100] * pad_len\n",
        "        else:\n",
        "            token_ids = token_ids[:self.max_len]\n",
        "            tag_list  = tag_list[:self.max_len]\n",
        "\n",
        "        return torch.tensor(token_ids), torch.tensor(tag_list)\n",
        "\n",
        "MAX_LEN = 100\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_ds = NerDataset(train_sentences, train_tags, vocab, MAX_LEN)\n",
        "dev_ds   = NerDataset(dev_sentences, dev_tags, vocab, MAX_LEN)\n",
        "test_ds  = NerDataset(test_sentences, test_tags, vocab, MAX_LEN)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dev_loader   = DataLoader(dev_ds, batch_size=BATCH_SIZE)\n",
        "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POap-TJWV_yS",
        "outputId": "5554d69c-076a-4f88-bd76-34e1d8e622ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kích thước từ điển: 5243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_SIZE = 256\n",
        "N_LAYERS = 5\n",
        "LR = 0.001\n",
        "EPOCHS = 20\n",
        "\n",
        "class BiLSTM_NER(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, n_tags):\n",
        "        super(BiLSTM_NER, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.2\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_size * 2, n_tags)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        out = self.dropout(lstm_out)\n",
        "        logits = self.classifier(out)\n",
        "        return logits\n",
        "\n",
        "model = BiLSTM_NER(vocab_size, EMBEDDING_DIM, HIDDEN_SIZE, N_LAYERS, num_tags)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "def train_and_evaluate_ner(model, train_loader, dev_loader, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for tokens, tags in train_loader:\n",
        "            tokens, tags = tokens.to(device), tags.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(tokens)\n",
        "\n",
        "            loss = criterion(outputs.view(-1, num_tags), tags.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        true_labels = []\n",
        "        pred_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for tokens, tags in dev_loader:\n",
        "                tokens, tags = tokens.to(device), tags.to(device)\n",
        "                outputs = model(tokens)\n",
        "                preds = torch.argmax(outputs, dim=2)\n",
        "\n",
        "                preds_np = preds.cpu().numpy()\n",
        "                tags_np = tags.cpu().numpy()\n",
        "\n",
        "                for i in range(len(tags_np)):\n",
        "                    temp_true = []\n",
        "                    temp_pred = []\n",
        "                    for j in range(len(tags_np[i])):\n",
        "                        if tags_np[i][j] != -100:\n",
        "                            temp_true.append(tag_names[tags_np[i][j]])\n",
        "                            temp_pred.append(tag_names[preds_np[i][j]])\n",
        "\n",
        "                    true_labels.append(temp_true)\n",
        "                    pred_labels.append(temp_pred)\n",
        "\n",
        "        dev_f1 = ner_f1_score(true_labels, pred_labels)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"Train Loss: {avg_loss:.4f}\")\n",
        "        print(f\"F1: {dev_f1:.12f}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "train_and_evaluate_ner(model, train_loader, dev_loader, epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoaqR7nlWHMu",
        "outputId": "33b6617c-13f5-4d0a-cee7-df03d771b95c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 1.0896\n",
            "F1: 0.000000000000\n",
            "------------------------------\n",
            "Epoch 2/20\n",
            "Train Loss: 0.5650\n",
            "F1: 0.539579762585\n",
            "------------------------------\n",
            "Epoch 3/20\n",
            "Train Loss: 0.2106\n",
            "F1: 0.760666893501\n",
            "------------------------------\n",
            "Epoch 4/20\n",
            "Train Loss: 0.1228\n",
            "F1: 0.826672076824\n",
            "------------------------------\n",
            "Epoch 5/20\n",
            "Train Loss: 0.0832\n",
            "F1: 0.848814025438\n",
            "------------------------------\n",
            "Epoch 6/20\n",
            "Train Loss: 0.0614\n",
            "F1: 0.855394656333\n",
            "------------------------------\n",
            "Epoch 7/20\n",
            "Train Loss: 0.0510\n",
            "F1: 0.860454147295\n",
            "------------------------------\n",
            "Epoch 8/20\n",
            "Train Loss: 0.0380\n",
            "F1: 0.861620540180\n",
            "------------------------------\n",
            "Epoch 9/20\n",
            "Train Loss: 0.0318\n",
            "F1: 0.876703134439\n",
            "------------------------------\n",
            "Epoch 10/20\n",
            "Train Loss: 0.0235\n",
            "F1: 0.869196608801\n",
            "------------------------------\n",
            "Epoch 11/20\n",
            "Train Loss: 0.0202\n",
            "F1: 0.875763747454\n",
            "------------------------------\n",
            "Epoch 12/20\n",
            "Train Loss: 0.0168\n",
            "F1: 0.872163439143\n",
            "------------------------------\n",
            "Epoch 13/20\n",
            "Train Loss: 0.0167\n",
            "F1: 0.876249751705\n",
            "------------------------------\n",
            "Epoch 14/20\n",
            "Train Loss: 0.0171\n",
            "F1: 0.878789943449\n",
            "------------------------------\n",
            "Epoch 15/20\n",
            "Train Loss: 0.0127\n",
            "F1: 0.874024373545\n",
            "------------------------------\n",
            "Epoch 16/20\n",
            "Train Loss: 0.0096\n",
            "F1: 0.885661939336\n",
            "------------------------------\n",
            "Epoch 17/20\n",
            "Train Loss: 0.0084\n",
            "F1: 0.891090452602\n",
            "------------------------------\n",
            "Epoch 18/20\n",
            "Train Loss: 0.0077\n",
            "F1: 0.882160283404\n",
            "------------------------------\n",
            "Epoch 19/20\n",
            "Train Loss: 0.0065\n",
            "F1: 0.892495755518\n",
            "------------------------------\n",
            "Epoch 20/20\n",
            "Train Loss: 0.0049\n",
            "F1: 0.891930347585\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_ner(model, test_loader, tag_names):\n",
        "    model.eval()\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    print(\"Đánh giá trên tập test:\")\n",
        "    with torch.no_grad():\n",
        "        for tokens, tags in test_loader:\n",
        "            tokens, tags = tokens.to(device), tags.to(device)\n",
        "            outputs = model(tokens)\n",
        "            preds = torch.argmax(outputs, dim=2)\n",
        "            preds = preds.cpu().numpy()\n",
        "            tags = tags.cpu().numpy()\n",
        "\n",
        "            for i in range(len(tags)):\n",
        "                temp_true = []\n",
        "                temp_pred = []\n",
        "                for j in range(len(tags[i])):\n",
        "                    if tags[i][j] != -100:\n",
        "                        temp_true.append(tag_names[tags[i][j]])\n",
        "                        temp_pred.append(tag_names[preds[i][j]])\n",
        "\n",
        "                true_labels.append(temp_true)\n",
        "                pred_labels.append(temp_pred)\n",
        "\n",
        "    f1 = ner_f1_score(true_labels, pred_labels)\n",
        "    print(f\"F1-Score: {f1:.4f}\")\n",
        "    print(ner_report(true_labels, pred_labels, digits=4))\n",
        "\n",
        "evaluate_ner(model, test_loader, tag_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaS4IVZHWj7O",
        "outputId": "800fd9cb-a3a2-4fd1-c39d-a2b1c87edb42"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đánh giá trên tập test:\n",
            "F1-Score: 0.8792\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "                AGE     0.9354    0.9322    0.9338       575\n",
            "               DATE     0.9509    0.9624    0.9566      1650\n",
            "             GENDER     0.9251    0.9231    0.9241       455\n",
            "                JOB     0.4962    0.3757    0.4276       173\n",
            "           LOCATION     0.8879    0.8713    0.8795      4435\n",
            "               NAME     0.8629    0.5346    0.6602       318\n",
            "       ORGANIZATION     0.7938    0.7938    0.7938       771\n",
            "         PATIENT_ID     0.9545    0.9286    0.9414      1988\n",
            "SYMPTOM_AND_DISEASE     0.8250    0.7386    0.7794      1136\n",
            "     TRANSPORTATION     0.9118    0.8031    0.8540       193\n",
            "\n",
            "          micro avg     0.8958    0.8633    0.8792     11694\n",
            "          macro avg     0.8543    0.7863    0.8150     11694\n",
            "       weighted avg     0.8935    0.8633    0.8768     11694\n",
            "\n"
          ]
        }
      ]
    }
  ]
}