{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiEvHNvD58Us"
      },
      "source": [
        "### Bài 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ut5xGzs640f",
        "outputId": "580a4ff4-63f4-449a-954d-021ee48f16a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from pyvi) (1.6.1)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pyvi) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pyvi) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pyvi) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->pyvi) (3.6.0)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.12/dist-packages (from sklearn-crfsuite->pyvi) (4.67.1)\n",
            "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVGrlg4b5_qn",
        "outputId": "81e6f4aa-525f-4ffa-992b-a3b62b5426aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from pyvi import ViTokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/MyDrive/UIT-VSFC data'\n",
        "\n",
        "train_df = pd.read_json(f'{base_path}/UIT-VSFC-train.json')\n",
        "dev_df   = pd.read_json(f'{base_path}/UIT-VSFC-dev.json')\n",
        "test_df  = pd.read_json(f'{base_path}/UIT-VSFC-test.json')\n",
        "\n",
        "labelid = {\n",
        "    \"negative\": 0,\n",
        "    \"neutral\": 1,\n",
        "    \"positive\": 2\n",
        "}\n",
        "\n",
        "y_train = train_df[\"sentiment\"].map(labelid).values\n",
        "y_dev   = dev_df[\"sentiment\"].map(labelid).values\n",
        "y_test  = test_df[\"sentiment\"].map(labelid).values\n",
        "\n",
        "X_train_text = train_df['sentence'].values\n",
        "X_dev_text   = dev_df['sentence'].values\n",
        "X_test_text  = test_df['sentence'].values\n",
        "\n",
        "def preprocess_text(text_list):\n",
        "    return [ViTokenizer.tokenize(str(text)) for text in text_list]\n",
        "\n",
        "X_train_text = preprocess_text(X_train_text)\n",
        "X_dev_text   = preprocess_text(X_dev_text)\n",
        "X_test_text  = preprocess_text(X_test_text)\n",
        "\n",
        "vocab_size = 12000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train_text)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_text)\n",
        "X_dev_seq   = tokenizer.texts_to_sequences(X_dev_text)\n",
        "X_test_seq  = tokenizer.texts_to_sequences(X_test_text)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='pre')\n",
        "X_dev_pad   = pad_sequences(X_dev_seq, maxlen=max_len, padding='pre')\n",
        "X_test_pad  = pad_sequences(X_test_seq, maxlen=max_len, padding='pre')\n",
        "\n",
        "X_train_pad = np.array(X_train_pad).astype(np.int64)\n",
        "X_dev_pad   = np.array(X_dev_pad).astype(np.int64)\n",
        "X_test_pad  = np.array(X_test_pad).astype(np.int64)\n",
        "\n",
        "y_train = np.array(y_train).astype(np.int64)\n",
        "y_dev   = np.array(y_dev).astype(np.int64)\n",
        "y_test  = np.array(y_test).astype(np.int64)\n",
        "\n",
        "train_data = TensorDataset(torch.tensor(X_train_pad), torch.tensor(y_train))\n",
        "dev_data   = TensorDataset(torch.tensor(X_dev_pad), torch.tensor(y_dev))\n",
        "test_data  = TensorDataset(torch.tensor(X_test_pad), torch.tensor(y_test))\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "dev_loader   = DataLoader(dev_data, batch_size=batch_size)\n",
        "test_loader  = DataLoader(test_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "12WYPQIp7oBN"
      },
      "outputs": [],
      "source": [
        "class GruModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers, n_labels, padding_idx=0):\n",
        "        super(GruModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=n_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.2\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_size, n_labels)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        gru_out, hidden = self.gru(embeds)\n",
        "        last_hidden = hidden[-1]\n",
        "        out = self.dropout(last_hidden)\n",
        "        logits = self.classifier(out)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8NEynGg71xe",
        "outputId": "cab5bd3d-751d-4b5b-c49b-134d12eb3c00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.4467\n",
            "F1: 0.87664920\n",
            "------------------------------\n",
            "Epoch 2/20\n",
            "Train Loss: 0.2904\n",
            "F1: 0.89418373\n",
            "------------------------------\n",
            "Epoch 3/20\n",
            "Train Loss: 0.2361\n",
            "F1: 0.90566194\n",
            "------------------------------\n",
            "Epoch 4/20\n",
            "Train Loss: 0.2071\n",
            "F1: 0.90605908\n",
            "------------------------------\n",
            "Epoch 5/20\n",
            "Train Loss: 0.1746\n",
            "F1: 0.91588290\n",
            "------------------------------\n",
            "Epoch 6/20\n",
            "Train Loss: 0.1435\n",
            "F1: 0.91163389\n",
            "------------------------------\n",
            "Epoch 7/20\n",
            "Train Loss: 0.1277\n",
            "F1: 0.91701347\n",
            "------------------------------\n",
            "Epoch 8/20\n",
            "Train Loss: 0.1038\n",
            "F1: 0.91775013\n",
            "------------------------------\n",
            "Epoch 9/20\n",
            "Train Loss: 0.0991\n",
            "F1: 0.91452146\n",
            "------------------------------\n",
            "Epoch 10/20\n",
            "Train Loss: 0.0838\n",
            "F1: 0.89981959\n",
            "------------------------------\n",
            "Epoch 11/20\n",
            "Train Loss: 0.0786\n",
            "F1: 0.92178917\n",
            "------------------------------\n",
            "Epoch 12/20\n",
            "Train Loss: 0.0588\n",
            "F1: 0.91342630\n",
            "------------------------------\n",
            "Epoch 13/20\n",
            "Train Loss: 0.0588\n",
            "F1: 0.91659312\n",
            "------------------------------\n",
            "Epoch 14/20\n",
            "Train Loss: 0.0546\n",
            "F1: 0.92396767\n",
            "------------------------------\n",
            "Epoch 15/20\n",
            "Train Loss: 0.0525\n",
            "F1: 0.91801227\n",
            "------------------------------\n",
            "Epoch 16/20\n",
            "Train Loss: 0.0498\n",
            "F1: 0.91104025\n",
            "------------------------------\n",
            "Epoch 17/20\n",
            "Train Loss: 0.0459\n",
            "F1: 0.91758155\n",
            "------------------------------\n",
            "Epoch 18/20\n",
            "Train Loss: 0.0394\n",
            "F1: 0.91797683\n",
            "------------------------------\n",
            "Epoch 19/20\n",
            "Train Loss: 0.0393\n",
            "F1: 0.91029260\n",
            "------------------------------\n",
            "Epoch 20/20\n",
            "Train Loss: 0.0374\n",
            "F1: 0.90881655\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "VOCAB_SIZE = vocab_size\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_SIZE = 256\n",
        "N_LAYERS = 5\n",
        "N_LABELS = 3\n",
        "LR = 0.001\n",
        "epochs=20\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = GruModel(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_SIZE, N_LAYERS, N_LABELS)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_and_evaluate(model, train_loader, dev_loader, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for text, labels in train_loader:\n",
        "            text, labels = text.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for text, labels in dev_loader:\n",
        "                text, labels = text.to(device), labels.to(device)\n",
        "                outputs = model(text)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        epoch_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
        "        print(f\"F1: {epoch_f1:.8f}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "train_and_evaluate(model, train_loader, dev_loader, epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt0Bqafy73q-",
        "outputId": "3b5c14bc-428d-47e8-d799-60b4656df35f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đánh giá trên tập test:\n",
            "F1-Score (Weighted): 0.8952\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Tiêu cực       0.91      0.92      0.92      1409\n",
            "  Trung tính       0.47      0.46      0.47       167\n",
            "    Tích cực       0.92      0.92      0.92      1590\n",
            "\n",
            "    accuracy                           0.90      3166\n",
            "   macro avg       0.77      0.77      0.77      3166\n",
            "weighted avg       0.89      0.90      0.90      3166\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text, labels in test_loader:\n",
        "            text, labels = text.to(device), labels.to(device)\n",
        "            outputs = model(text)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "    print(f\"Đánh giá trên tập test:\")\n",
        "    print(f\"F1-Score (Weighted): {f1:.4f}\")\n",
        "    print(classification_report(all_labels, all_preds, target_names=['Tiêu cực', 'Trung tính', 'Tích cực']))\n",
        "\n",
        "evaluate_model(model, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
